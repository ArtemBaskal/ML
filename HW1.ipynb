{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNE1Eh8kRMMFgZNpCOaGFY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtemBaskal/ML/blob/main/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxlMj0-7uZVP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn.linear_model\n",
        "import sklearn.model_selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading 20news dataset.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "data = fetch_20newsgroups()\n",
        "print(data)"
      ],
      "metadata": {
        "id": "PLhPJG7uueCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4ab92b-d53b-4178-9651-ebe733ff4371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_cFWCgD2JLdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use HashingVectorizer to encode the text into sparse features:\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Assume there are 1 000 000 unique words in English language\n",
        "# Linear models are good for sparse text data, and it also the fastest solution, which gives a baseline\n",
        "vectorizer = HashingVectorizer(n_features=10 ** 6, binary=True, norm=None)\n",
        "features = vectorizer.fit_transform(data.data)"
      ],
      "metadata": {
        "id": "zEFXfLYUugEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the K-Fold cross-validation to split the dataset into training and test parts:\n",
        "\n",
        "kf = sklearn.model_selection.KFold(n_splits=3, shuffle=True, random_state=9)\n",
        "features_train, features_test, target_train, target_test\n",
        "\n",
        "# Checking for overfitting and splitting the dataset\n",
        "for train_index, test_index in kf.split(features):\n",
        "  features_train, features_test = features[train_index], features[test_index]\n",
        "  target_train, target_test = data.target[train_index], data.target[test_index]"
      ],
      "metadata": {
        "id": "d0bC8VeauhIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different models (L1, L2)\n",
        "\n",
        "# l1 forces properties to be zero, use it if some features are useless\n",
        "classification_model_l1 = sklearn.linear_model.SGDClassifier(loss='log', penalty='l1')\n",
        "classification_model_l1.fit(features_train, target_train)\n",
        "predicted_l1 = classification_model_l1.predict(features_test)\n",
        "\n",
        "# l2 forces features to be equaly important (the same magnitude)\n",
        "classification_model_l2 = sklearn.linear_model.SGDClassifier(loss='log', penalty='l2')\n",
        "classification_model_l2.fit(features_train, target_train)\n",
        "predicted_l2 = classification_model_l2.predict(features_test)"
      ],
      "metadata": {
        "id": "OVhCj5TnuioD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What model worked best?\n",
        "from sklearn import metrics\n",
        "\n",
        "print('--- l1 penalty ---')\n",
        "print(metrics.classification_report(target_test, predicted_l1, target_names=data.target_names))\n",
        "print(metrics.confusion_matrix(target_test, predicted_l1))\n",
        "print(f'We achieved {np.mean(predicted_l1 == target_test) * 100:.2f}% accuracy with l1 penalty')\n",
        "\n",
        "print('\\n--- l2 penalty ---')\n",
        "print(metrics.classification_report(target_test, predicted_l2, target_names=data.target_names))\n",
        "print(metrics.confusion_matrix(target_test, predicted_l2))\n",
        "print(f'We achieved {np.mean(predicted_l2 == target_test) * 100:.2f}% accuracy with l2 penalty')\n",
        "# In average l2 variant is more precise as words are eually important"
      ],
      "metadata": {
        "id": "THjMiYEVuj_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "ed34f43b-7428-4ce2-9d62-3d348f60652f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- l1 penalty ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7898c1378623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- l1 penalty ---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_l1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'We achieved {np.mean(predicted_l1 == target_test) * 100:.2f}% accuracy with l1 penalty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target_test' is not defined"
          ]
        }
      ]
    }
  ]
}